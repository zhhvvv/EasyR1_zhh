# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Confidence Calibration Reward for RL Training (FINAL VERSION)

ä½¿ç”¨ç»å¯¹Top-1æ¦‚ç‡ï¼ˆä¿®æ­£ç‰ˆï¼‰ï¼š
1. âœ… ä½¿ç”¨top-1çš„ç»å¯¹æ¦‚ç‡ï¼ˆä¸åšå½’ä¸€åŒ–ï¼‰
2. âœ… å¯¹ç§°çš„å››è±¡é™rewardè®¾è®¡ï¼ˆV4ï¼‰
3. âœ… å®Œæ•´çš„æ ¡å‡†æŒ‡æ ‡ç›‘æ§

å…³é”®ä¿®æ­£ï¼š
ä¹‹å‰ç”¨å½’ä¸€åŒ–ä¼šé«˜ä¼°confidenceï¼ˆå› ä¸ºå¿½ç•¥top-5å¤–çš„æ¦‚ç‡ï¼‰
ç°åœ¨ç”¨ç»å¯¹æ¦‚ç‡ï¼ŒçœŸå®åæ˜ æ¨¡å‹ä¸ç¡®å®šæ€§
"""

import re
import numpy as np
from typing import Any, Dict, List, Optional
from mathruler.grader import extract_boxed_content, grade_answer

# Metadata
REWARD_NAME = "math_with_entropy"
REWARD_TYPE = "batch"

# Debug flag
DEBUG = True
DEBUG_SAMPLE_COUNT = 3


def format_reward(response: str) -> float:
    """Check format"""
    pattern = re.compile(r"<think>.*</think>.*\\boxed\{.*\}.*", re.DOTALL)
    format_match = re.fullmatch(pattern, response)
    return 1.0 if format_match else 0.0


def accuracy_reward(response: str, ground_truth: str) -> float:
    """Check accuracy"""
    answer = extract_boxed_content(response)
    return 1.0 if grade_answer(answer, ground_truth) else 0.0


def extract_logprob_values(vllm_logprobs_dict: Dict) -> Dict[int, float]:
    """
    ä» vLLM çš„ Logprob å¯¹è±¡ä¸­æå–çœŸæ­£çš„ logprob å€¼
    
    Args:
        vllm_logprobs_dict: vLLM è¿”å›çš„æ¯ä¸ªtokenä½ç½®çš„ logprobs (top-5)
    
    Returns:
        {token_id: logprob_value}
    """
    if not vllm_logprobs_dict:
        return {}
    
    logprobs_dict = {}
    for token_id, logprob_obj in vllm_logprobs_dict.items():
        if hasattr(logprob_obj, 'logprob'):
            logprobs_dict[token_id] = logprob_obj.logprob
        else:
            logprobs_dict[token_id] = float(logprob_obj)
    
    return logprobs_dict


def get_token_confidence_absolute(logprobs_dict: Dict[int, float]) -> float:
    """
    â­â­â­ æœ€ç»ˆæ¨èæ–¹æ³•: ç»å¯¹Top-1æ¦‚ç‡
    
    ç›´æ¥ä½¿ç”¨top-1çš„ç»å¯¹æ¦‚ç‡ï¼Œä¸åšå½’ä¸€åŒ–
    
    ä¸ºä»€ä¹ˆè¿™æ ·æ›´å¥½ï¼š
    1. çœŸå®åæ˜ æ¨¡å‹ä¸ç¡®å®šæ€§ï¼ˆä¸ä¼šå› ä¸ºåªçœ‹top-5è€Œé«˜ä¼°ï¼‰
    2. æ•°å­¦ä¸Šæ­£ç¡®ï¼ˆæ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡å·²ç»å¯¹å…¨è¯è¡¨å½’ä¸€åŒ–ï¼‰
    3. é¿å…å½’ä¸€åŒ–å¸¦æ¥çš„ç³»ç»Ÿæ€§é«˜ä¼°ï¼ˆå½“top-5å¤–æœ‰è¾ƒå¤šæ¦‚ç‡æ—¶ï¼‰
    
    ä¾‹å­ï¼š
    - å¦‚æœtop-1å 70%çš„æ¦‚ç‡ â†’ confidence = 0.70
    - å¦‚æœtop-1åªå 20%ï¼ˆå› ä¸ºæœ‰å¾ˆå¤šå…¶ä»–é€‰é¡¹ï¼‰â†’ confidence = 0.20
    
    å¯¹æ¯”å½’ä¸€åŒ–æ–¹æ³•ï¼š
    - åœºæ™¯ï¼štop-1=0.37, top-5æ€»å’Œ=0.58ï¼ˆtop-5å¤–æœ‰42%ï¼‰
    - å½’ä¸€åŒ–ï¼š0.37/0.58 = 0.64 âŒï¼ˆé«˜ä¼°73%ï¼‰
    - ç»å¯¹å€¼ï¼š0.37 âœ…ï¼ˆçœŸå®å€¼ï¼‰
    
    Args:
        logprobs_dict: {token_id: logprob} for top-5 tokens
    
    Returns:
        confidence: float in [0, 1], top-1çš„ç»å¯¹æ¦‚ç‡
    """
    if not logprobs_dict:
        return 0.0
    
    # ç›´æ¥ç”¨top-1çš„logprob
    top1_logprob = list(logprobs_dict.values())[0]
    
    # è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆç»å¯¹å€¼ï¼Œä¸å½’ä¸€åŒ–ï¼‰
    confidence = float(np.exp(top1_logprob))
    
    return confidence


def calculate_response_confidence(
    response_logprobs: List[Dict], 
    debug_info: dict = None,
    aggregation_method: str = "mean"
) -> Optional[float]:
    """
    è®¡ç®—æ•´ä¸ªresponseçš„confidenceï¼ˆä½¿ç”¨ç»å¯¹æ¦‚ç‡ï¼‰
    
    æ­¥éª¤ï¼š
    1. å¯¹æ¯ä¸ªtokenï¼Œä½¿ç”¨top-1çš„ç»å¯¹æ¦‚ç‡ä½œä¸ºconfidence
    2. èšåˆæ‰€æœ‰tokençš„confidenceï¼ˆé»˜è®¤ï¼šç®—æœ¯å¹³å‡ï¼‰
    
    Args:
        response_logprobs: vLLM è¿”å›çš„ response ä¸­æ‰€æœ‰ logprobs åˆ—è¡¨
        debug_info: ç”¨äºæ”¶é›†è°ƒè¯•ä¿¡æ¯çš„å­—å…¸ï¼ˆå¯é€‰ï¼‰
        aggregation_method: èšåˆæ–¹æ³•ï¼Œå¯é€‰ "mean"ï¼ˆæ¨èï¼‰, "geometric", "min"
    
    Returns:
        æ•´ä¸ª response çš„ confidence [0, 1]
    """
    if not response_logprobs:
        if debug_info is not None:
            debug_info["error"] = "response_logprobs is empty or None"
        return None
    
    if debug_info is not None:
        debug_info["num_tokens"] = len(response_logprobs)
        debug_info["first_token_logprobs_type"] = str(type(response_logprobs[0]))
        debug_info["first_token_logprobs_keys"] = list(response_logprobs[0].keys())[:5] if response_logprobs[0] else "empty"
    
    token_confidences = []
    
    for i, vllm_logprobs in enumerate(response_logprobs):
        # æå–logprobå€¼
        logprobs_dict = extract_logprob_values(vllm_logprobs)
        
        if debug_info is not None and i == 0:
            debug_info["first_token_extracted_logprobs"] = {k: float(v) for k, v in list(logprobs_dict.items())[:5]}
        
        if logprobs_dict:
            # ä½¿ç”¨ç»å¯¹æ¦‚ç‡æ–¹æ³•
            token_conf = get_token_confidence_absolute(logprobs_dict)
            token_confidences.append(token_conf)
            
            if debug_info is not None and i < 3:
                if "sample_token_confidences" not in debug_info:
                    debug_info["sample_token_confidences"] = []
                debug_info["sample_token_confidences"].append(float(token_conf))
    
    if not token_confidences:
        if debug_info is not None:
            debug_info["error"] = "No valid token confidences calculated"
        return None
    
    # èšåˆtokençº§åˆ«çš„confidence
    if aggregation_method == "mean":
        response_confidence = np.mean(token_confidences)
    elif aggregation_method == "geometric":
        # å‡ ä½•å¹³å‡ï¼Œå¯¹æç«¯ä½å€¼æ›´æ•æ„Ÿ
        response_confidence = np.exp(np.mean(np.log(np.array(token_confidences) + 1e-10)))
    elif aggregation_method == "min":
        # æœ€ä¿å®ˆï¼šå–æœ€å°å€¼
        response_confidence = np.min(token_confidences)
    else:
        response_confidence = np.mean(token_confidences)
    
    if debug_info is not None:
        debug_info["num_valid_tokens"] = len(token_confidences)
        debug_info["response_confidence"] = float(response_confidence)
        debug_info["confidence_min"] = float(np.min(token_confidences))
        debug_info["confidence_max"] = float(np.max(token_confidences))
        debug_info["confidence_std"] = float(np.std(token_confidences))
        debug_info["aggregation_method"] = aggregation_method
    
    return response_confidence


def confidence_reward_v4(is_correct: bool, confidence: float) -> float:
    """
    â­ å¯¹ç§°çš„å››è±¡é™æ ¡å‡†reward
    
    è®¾è®¡ç†å¿µï¼š
    - åœ¨confidence=0.5æ—¶ä¸ºä¸­æ€§ç‚¹ï¼ˆreward=0ï¼‰
    - æ­£ç¡®ç­”æ¡ˆï¼šé¼“åŠ±confidence>0.5ï¼Œæƒ©ç½šconfidence<0.5
    - é”™è¯¯ç­”æ¡ˆï¼šé¼“åŠ±confidence<0.5ï¼Œæƒ©ç½šconfidence>0.5
    - å®Œå…¨å¯¹ç§°ï¼Œé¿å…æ¨¡å‹åå‘æŸä¸€ç­–ç•¥
    
    æ•°å­¦å½¢å¼ï¼š
    - æ­£ç¡®æ—¶ï¼šreward = 2 * (confidence - 0.5) âˆˆ [-1.0, +1.0]
    - é”™è¯¯æ—¶ï¼šreward = 2 * (0.5 - confidence) âˆˆ [-1.0, +1.0]
    """
    if is_correct:
        return 2.0 * (confidence - 0.5)
    else:
        return 2.0 * (0.5 - confidence)


def calculate_calibration_metrics(scores: List[Dict]) -> Dict[str, float]:
    """è®¡ç®—æ ¡å‡†æŒ‡æ ‡"""
    correct_confidences = [s["confidence_value"] for s in scores 
                          if s["accuracy"] == 1.0 and s["confidence_value"] > 0]
    wrong_confidences = [s["confidence_value"] for s in scores 
                        if s["accuracy"] == 0.0 and s["confidence_value"] > 0]
    
    all_confidences = [s["confidence_value"] for s in scores if s["confidence_value"] > 0]
    all_accuracies = [s["accuracy"] for s in scores if s["confidence_value"] > 0]
    
    metrics = {}
    
    # 1. Separationï¼ˆåˆ†ç¦»åº¦ï¼‰
    if correct_confidences and wrong_confidences:
        metrics["separation"] = np.mean(correct_confidences) - np.mean(wrong_confidences)
    else:
        metrics["separation"] = 0.0
    
    # 2. ECE (Expected Calibration Error)
    if all_confidences and all_accuracies:
        n_bins = 10
        bins = np.linspace(0, 1, n_bins + 1)
        ece = 0.0
        
        confidences_arr = np.array(all_confidences)
        accuracies_arr = np.array(all_accuracies)
        
        for i in range(n_bins):
            bin_mask = (confidences_arr >= bins[i]) & (confidences_arr < bins[i+1])
            if bin_mask.sum() > 0:
                bin_acc = accuracies_arr[bin_mask].mean()
                bin_conf = confidences_arr[bin_mask].mean()
                ece += bin_mask.sum() / len(all_confidences) * abs(bin_acc - bin_conf)
        
        metrics["ece"] = ece
    else:
        metrics["ece"] = 0.0
    
    # 3. Brier Score
    if all_confidences and all_accuracies:
        brier = np.mean([(c - a)**2 for c, a in zip(all_confidences, all_accuracies)])
        metrics["brier_score"] = brier
    else:
        metrics["brier_score"] = 0.0
    
    # 4. Sharpness (åŒºåˆ†åº¦)
    if all_confidences:
        metrics["sharpness"] = np.std(all_confidences)
    else:
        metrics["sharpness"] = 0.0
    
    # 5. å„ç»„ç»Ÿè®¡
    if correct_confidences:
        metrics["correct_conf_mean"] = np.mean(correct_confidences)
        metrics["correct_conf_std"] = np.std(correct_confidences)
    else:
        metrics["correct_conf_mean"] = 0.0
        metrics["correct_conf_std"] = 0.0
    
    if wrong_confidences:
        metrics["wrong_conf_mean"] = np.mean(wrong_confidences)
        metrics["wrong_conf_std"] = np.std(wrong_confidences)
    else:
        metrics["wrong_conf_mean"] = 0.0
        metrics["wrong_conf_std"] = 0.0
    
    return metrics


def build_confidence_confusion_matrix(
    scores: List[Dict],
    high_confidence_threshold: float = 0.5
) -> Dict[str, Any]:
    """æ„å»ºç½®ä¿¡åº¦æ··æ·†çŸ©é˜µï¼ˆä»…ç”¨äºç»Ÿè®¡ï¼‰"""
    correct_high_conf = 0
    correct_low_conf = 0
    wrong_high_conf = 0
    wrong_low_conf = 0
    
    correct_high_conf_list = []
    correct_low_conf_list = []
    wrong_high_conf_list = []
    wrong_low_conf_list = []
    
    for s in scores:
        is_correct = (s["accuracy"] == 1.0)
        confidence = s["confidence_value"]
        is_high_conf = confidence >= high_confidence_threshold
        
        if confidence == 0:
            continue
        
        if is_correct and is_high_conf:
            correct_high_conf += 1
            correct_high_conf_list.append(confidence)
        elif is_correct and not is_high_conf:
            correct_low_conf += 1
            correct_low_conf_list.append(confidence)
        elif not is_correct and is_high_conf:
            wrong_high_conf += 1
            wrong_high_conf_list.append(confidence)
        else:
            wrong_low_conf += 1
            wrong_low_conf_list.append(confidence)
    
    total = correct_high_conf + correct_low_conf + wrong_high_conf + wrong_low_conf
    
    matrix = {
        "total_samples": total,
        "correct_high_conf": {
            "count": correct_high_conf,
            "percentage": correct_high_conf / total * 100 if total > 0 else 0,
            "avg_confidence": np.mean(correct_high_conf_list) if correct_high_conf_list else 0,
        },
        "correct_low_conf": {
            "count": correct_low_conf,
            "percentage": correct_low_conf / total * 100 if total > 0 else 0,
            "avg_confidence": np.mean(correct_low_conf_list) if correct_low_conf_list else 0,
        },
        "wrong_high_conf": {
            "count": wrong_high_conf,
            "percentage": wrong_high_conf / total * 100 if total > 0 else 0,
            "avg_confidence": np.mean(wrong_high_conf_list) if wrong_high_conf_list else 0,
        },
        "wrong_low_conf": {
            "count": wrong_low_conf,
            "percentage": wrong_low_conf / total * 100 if total > 0 else 0,
            "avg_confidence": np.mean(wrong_low_conf_list) if wrong_low_conf_list else 0,
        },
    }
    
    return matrix


def compute_score(
    reward_inputs: list[dict[str, Any]],
    format_weight: float = 0.1,
    confidence_weight: float = 0.15,
    confusion_matrix_threshold: float = 0.5,
    aggregation_method: str = "mean"
) -> list[dict[str, float]]:
    """
    è®¡ç®—å¥–åŠ±åˆ†æ•°ï¼ˆæœ€ç»ˆç‰ˆ - ä½¿ç”¨ç»å¯¹æ¦‚ç‡ï¼‰
    
    Args:
        reward_inputs: è¾“å…¥åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
            - "response": æ¨¡å‹ç”Ÿæˆçš„ response æ–‡æœ¬
            - "ground_truth": æ­£ç¡®ç­”æ¡ˆ
            - "response_logprobs": vLLM è¿”å›çš„ response logprobsï¼ˆtop-5ï¼‰
        format_weight: æ ¼å¼å¥–åŠ±æƒé‡
        confidence_weight: ç½®ä¿¡åº¦å¥–åŠ±æƒé‡
        confusion_matrix_threshold: æ··æ·†çŸ©é˜µçš„é˜ˆå€¼ï¼ˆä»…ç”¨äºç»Ÿè®¡ï¼Œä¸å½±å“è®­ç»ƒï¼‰
        aggregation_method: token confidenceèšåˆæ–¹æ³• ("mean", "geometric", "min")
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. â­ ä½¿ç”¨ç»å¯¹top-1æ¦‚ç‡ï¼ˆä¸å½’ä¸€åŒ–ï¼Œé¿å…é«˜ä¼°ï¼‰
    2. â­ å¯¹ç§°çš„å››è±¡é™rewardè®¾è®¡ï¼ˆconfidence_reward_v4ï¼‰
    3. â­ å®Œæ•´çš„æ ¡å‡†æŒ‡æ ‡ç›‘æ§
    """
    accuracy_weight = 1.0 - format_weight - confidence_weight
    
    if DEBUG:
        print("\n" + "="*80)
        print("DEBUG: compute_score called (FINAL VERSION - ABSOLUTE PROBABILITY)")
        print(f"Batch size: {len(reward_inputs)}")
        print(f"Weights: accuracy={accuracy_weight:.3f}, format={format_weight:.3f}, confidence={confidence_weight:.3f}")
        print(f"Aggregation method: {aggregation_method}")
        print(f"Confusion matrix threshold (for statistics only): {confusion_matrix_threshold}")
        print("\nğŸ¯ Confidence method: Absolute Top-1 Probability (ç»å¯¹top-1æ¦‚ç‡)")
        print("   â†’ ç›´æ¥ç”¨top-1çš„æ¦‚ç‡ï¼Œä¸åšå½’ä¸€åŒ–")
        print("   â†’ é¿å…å› åªçœ‹top-5è€Œé«˜ä¼°confidence")
        print("\nğŸ¯ Reward design: Symmetric Four-Quadrant (V4)")
        print("="*80 + "\n")
    
    scores = []
    for idx, reward_input in enumerate(reward_inputs):
        response = re.sub(r"\s*(<|>|/)\s*", r"\1", reward_input["response"])
        
        format_score = format_reward(response)
        accuracy_score = accuracy_reward(response, reward_input["ground_truth"])
        
        # è®¡ç®—æ•´ä¸ª response çš„ confidence
        response_logprobs = reward_input.get("response_logprobs", None)
        
        debug_info = {} if (DEBUG and idx < DEBUG_SAMPLE_COUNT) else None
        
        if response_logprobs is not None:
            response_confidence = calculate_response_confidence(
                response_logprobs, 
                debug_info,
                aggregation_method=aggregation_method
            )
            
            if response_confidence is not None:
                # ä½¿ç”¨V4å¯¹ç§°reward
                conf_reward = confidence_reward_v4(
                    is_correct=(accuracy_score == 1.0),
                    confidence=response_confidence
                )
                
                confidence_value = response_confidence
                
                # æ˜¯å¦æ ¡å‡†ï¼ˆä»…ç”¨äºç»Ÿè®¡ï¼‰
                is_calibrated = 1.0 if (
                    (accuracy_score == 1.0 and response_confidence >= confusion_matrix_threshold) or
                    (accuracy_score == 0.0 and response_confidence < confusion_matrix_threshold)
                ) else 0.0
                
                if DEBUG and idx < DEBUG_SAMPLE_COUNT:
                    print(f"\n--- Sample {idx} ---")
                    print(f"Response (first 100 chars): {response[:100]}...")
                    print(f"Accuracy: {accuracy_score}, Format: {format_score}")
                    print(f"Debug info: {debug_info}")
                    print(f"Response confidence (absolute): {response_confidence:.4f}")
                    print(f"Confidence reward (v4): {conf_reward:.4f}")
                    
                    # å±•ç¤ºrewardè®¾è®¡çš„æ•ˆæœ
                    if accuracy_score == 1.0:
                        print(f"  â†’ âœ… Correct answer, conf={response_confidence:.2f}")
                        print(f"     Reward range: [-1.00 (conf=0) to +1.00 (conf=1)]")
                        print(f"     Current: {conf_reward:.2f}")
                    else:
                        print(f"  â†’ âŒ Wrong answer, conf={response_confidence:.2f}")
                        print(f"     Reward range: [+1.00 (conf=0) to -1.00 (conf=1)]")
                        print(f"     Current: {conf_reward:.2f}")
            else:
                conf_reward = 0.0
                confidence_value = 0.0
                is_calibrated = 0.0
                
                if DEBUG and idx < DEBUG_SAMPLE_COUNT:
                    print(f"\n--- Sample {idx} (confidence calculation failed) ---")
                    print(f"Debug info: {debug_info}")
        else:
            conf_reward = 0.0
            confidence_value = 0.0
            is_calibrated = 0.0
            
            if DEBUG and idx < DEBUG_SAMPLE_COUNT:
                print(f"\n--- Sample {idx} (no logprobs) ---")
                print("response_logprobs is None!")
        
        # è®¡ç®—æ··æ·†çŸ©é˜µåˆ†ç±»
        is_correct = (accuracy_score == 1.0)
        is_high_conf = (confidence_value >= confusion_matrix_threshold)
        
        confusion_correct_high = 1.0 if (is_correct and is_high_conf) else 0.0
        confusion_correct_low = 1.0 if (is_correct and not is_high_conf) else 0.0
        confusion_wrong_high = 1.0 if (not is_correct and is_high_conf) else 0.0
        confusion_wrong_low = 1.0 if (not is_correct and not is_high_conf) else 0.0
        confusion_valid_sample = 1.0 if confidence_value > 0 else 0.0
        
        # è®¡ç®—æ€»åˆ†
        base_score = accuracy_weight * accuracy_score + format_weight * format_score
        overall_score = base_score + confidence_weight * conf_reward
        overall_score = max(0.0, min(1.0, overall_score))
        
        scores.append({
            "overall": overall_score,
            "format": format_score,
            "accuracy": accuracy_score,
            "confidence_calibration": conf_reward,
            "confidence_value": confidence_value,
            "is_calibrated": is_calibrated,
            "confusion_correct_high_conf": confusion_correct_high,
            "confusion_correct_low_conf": confusion_correct_low,
            "confusion_wrong_high_conf": confusion_wrong_high,
            "confusion_wrong_low_conf": confusion_wrong_low,
            "confusion_valid_sample": confusion_valid_sample,
        })
    
    if DEBUG:
        print("\n" + "="*80)
        print("BATCH STATISTICS:")
        
        all_confidences = [s["confidence_value"] for s in scores if s["confidence_value"] > 0]
        all_accuracies = [s["accuracy"] for s in scores]
        
        print(f"Valid samples: {len(all_confidences)} / {len(scores)}")
        if all_confidences:
            print(f"Confidence - mean: {np.mean(all_confidences):.4f}, std: {np.std(all_confidences):.4f}, "
                  f"min: {np.min(all_confidences):.4f}, max: {np.max(all_confidences):.4f}")
        print(f"Accuracy - mean: {np.mean(all_accuracies):.4f}")
        
        # æ ¡å‡†æŒ‡æ ‡
        print("\n" + "-"*80)
        print("CALIBRATION METRICS:")
        cal_metrics = calculate_calibration_metrics(scores)
        print(f"âœ“ Separation: {cal_metrics['separation']:.4f} (target: >0.30)")
        print(f"âœ“ ECE: {cal_metrics['ece']:.4f} (target: <0.10)")
        print(f"âœ“ Brier Score: {cal_metrics.get('brier_score', 0):.4f} (target: <0.15)")
        print(f"âœ“ Sharpness: {cal_metrics.get('sharpness', 0):.4f} (target: >0.20)")
        
        # æ··æ·†çŸ©é˜µ
        print("\n" + "-"*80)
        conf_matrix = build_confidence_confusion_matrix(scores, confusion_matrix_threshold)
        print(f"CONFIDENCE CONFUSION MATRIX (threshold={confusion_matrix_threshold}):")
        
        total = conf_matrix['total_samples']
        if total > 0:
            good_calib = conf_matrix['correct_high_conf']['percentage'] + conf_matrix['wrong_low_conf']['percentage']
            print(f"âœ… Correct+High: {conf_matrix['correct_high_conf']['percentage']:.1f}%")
            print(f"âŒ Correct+Low:  {conf_matrix['correct_low_conf']['percentage']:.1f}%")
            print(f"âŒ Wrong+High:   {conf_matrix['wrong_high_conf']['percentage']:.1f}%")
            print(f"âœ… Wrong+Low:    {conf_matrix['wrong_low_conf']['percentage']:.1f}%")
            print(f"\nğŸ“Š Good Calibration: {good_calib:.1f}% (target: >90%)")
            
            if good_calib < 70:
                print("   âš ï¸  Needs more training")
            elif good_calib < 85:
                print("   ğŸ”„ Improving")
            else:
                print("   âœ… Well-calibrated!")
        
        print("="*80 + "\n")
    
    return scores
